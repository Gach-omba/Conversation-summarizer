{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlOqxFYrRYEd"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip uninstall accelerate transformers[torch] -y\n",
        "!pip install accelerate transformers[torch] -U\n",
        "!pip install rouge_score py7zr\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load and prepare the cnn_dailymail dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "print(f\"Features: {dataset['train'].column_names}\")\n",
        "print(f\"{len(dataset['test'])}\")\n",
        "print(f\"{dataset['train']['article'][0][:500]}\")\n",
        "print(f\"Summary: {dataset['train']['highlights'][0]}\")\n",
        "sample_text = dataset['train']['article'][1][:2000]\n"
      ],
      "metadata": {
        "id": "uUWq4HLnRcho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining baseline summary function\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def three_sentence_summary(text):\n",
        "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "\n",
        "from datasets import load_metric\n",
        "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)\n",
        "\n",
        "def evaluate_summaries_baseline(dataset, metric, column_text='article', column_summary='highlights'):\n",
        "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
        "    metric.add_batch(predictions=summaries, references=dataset[column_summary])\n",
        "    scores = metric.compute()\n",
        "    return scores\n",
        "\n",
        "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "uykx-vCYRxgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating summaries using pegasus\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def chunks(list_of_elements, batch_size):\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i:i+batch_size]\n",
        "\n",
        "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device,\n",
        "                               column_text=\"article\", column_summary=\"highlights\"):\n",
        "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
        "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
        "        inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        torch.cuda.empty_cache() # clear the cache at the end of each iteration to avoid running out of memory when training on a standard GPU\n",
        "        try:\n",
        "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                                       attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                                       length_penalty=0.8, num_beams=8, max_length=128)\n",
        "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
        "            decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
        "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "        except RuntimeError as e:\n",
        "            if 'out of memory' in str(e):\n",
        "                print(\"Out of memory error occurred. Skipping this batch.\")\n",
        "                torch.cuda.empty_cache()\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qWGIWXlSR7_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the samsum (conversations and their summaries dataset by Samsung)\n",
        "dataset_samsum = load_dataset(\"samsum\", trust_remote_code=True)\n",
        "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "print(dataset_samsum[\"test\"][0][\"summary\"])\n",
        "\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
        "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "print(\"Summary:\")\n",
        "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))\n"
      ],
      "metadata": {
        "id": "D110tTouSDzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing the conversations and their summary token distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
        "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
        "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
        "axes[0].set_title(\"Dialogue Token Length\")\n",
        "axes[0].set_xlabel(\"Length\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
        "axes[1].set_title(\"Summary Token Length\")\n",
        "axes[1].set_xlabel(\"Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eZK2_okjR9_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the tokens into features\n",
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True)\n",
        "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
        "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "            \"labels\": target_encodings[\"input_ids\"]}\n",
        "\n",
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)\n",
        "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
      ],
      "metadata": {
        "id": "evY52GfsSkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the data collator which will automatically handle padding to ensure uniformity of the input and output\n",
        "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "# defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum',\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16 # the model has one batch which makes it hard to converge therefore we introduce this gradient accumulator as an alternative way to calculate gradient\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "TIv19A2GT5hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=seq2seq_data_collator,\n",
        "    train_dataset=dataset_samsum_pt[\"train\"],\n",
        "    eval_dataset=dataset_samsum_pt[\"validation\"]\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3HmZ9h6wU2cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model\n",
        "score = evaluate_summaries_pegasus(\n",
        "    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
        "    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\"\n",
        ")\n",
        "\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "print(pd.DataFrame(rouge_dict, index=[f\"pegasus\"]))"
      ],
      "metadata": {
        "id": "kVU6uz2mU7P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model on the samsum dataset\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum\") # this is the name of your trained model\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\n"
      ],
      "metadata": {
        "id": "HvNUXIO-U8jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model on custom dialogue\n",
        "my_message=\"\"\"\\\n",
        "\n",
        "Person A: Hey everyone! How's it going?\n",
        "\n",
        "Person B: Hi! I'm doing well, just finished a big project at work. What about you?\n",
        "\n",
        "Person C: Hey! Congrats on finishing your project, B. I've been busy with some personal stuff, but it's all good now. How about you, A?\n",
        "\n",
        "Person A: Thanks, C! I'm doing great. Just got back from a short trip to the mountains. It was so refreshing. How are you doing, D?\n",
        "\n",
        "Person D: Hi all! I'm doing fine, just dealing with some house renovations. It's a bit chaotic, but I'm excited to see the final result.\n",
        "\n",
        "Person B: That sounds exciting, D. What kind of renovations are you doing?\n",
        "\n",
        "Person D: We're redoing the kitchen and adding a small patio in the backyard. It's a lot of work, but I think it'll be worth it.\n",
        "\n",
        "Person C: That sounds amazing! I love spending time outdoors, so a patio sounds perfect. Maybe we can have a get-together there once it's done?\n",
        "\n",
        "Person D: Absolutely! I'd love that. We could have a barbecue or something.\n",
        "\n",
        "Person A: Count me in! Speaking of get-togethers, we haven't had one in a while. Maybe we should plan something soon.\n",
        "\n",
        "Person B: Yes, we should. How about a game night? We could all bring our favorite games and snacks.\n",
        "\n",
        "Person C: I love that idea! I'm always up for a good game night. Let's set a date.\n",
        "\n",
        "Person D: How about next Saturday? Does that work for everyone?\n",
        "\n",
        "Person A: Next Saturday works for me. What about you, B and C?\n",
        "\n",
        "Person B: Works for me too!\n",
        "\n",
        "Person C: Same here! Looking forward to it.\n",
        "\n",
        "Person D: Great! It's a plan then. I'll make sure to have the patio ready by then.\n",
        "\n",
        "Person A: Awesome! This is going to be so much fun. Can't wait to see everyone.\n",
        "\n",
        "Person B: Me too! It's going to be a blast.\n",
        "\n",
        "Person C: Definitely. See you all next Saturday!\n",
        "\n",
        "Person D: See you then!\n",
        "\"\"\"\n",
        "print(pipe(my_message, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "eVl__wMkVoF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}